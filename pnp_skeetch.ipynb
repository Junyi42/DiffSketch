{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyi/miniconda3/envs/od/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from pytorch_lightning import seed_everything\n",
    "try:\n",
    "    from torch import autocast\n",
    "except:\n",
    "    from torch.cuda.amp import autocast\n",
    "from contextlib import nullcontext\n",
    "import json\n",
    "from torchvision import transforms\n",
    "import logging\n",
    "from pnp_utils import check_safety\n",
    "\n",
    "from pnp_ldm.util import instantiate_from_config\n",
    "from pnp_ldm.models.diffusion.ddim import DDIMSampler\n",
    "import collections\n",
    "\n",
    "memory_storage = collections.defaultdict(dict)\n",
    "\n",
    "def load_img(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    x, y = image.size\n",
    "    print(f\"loaded input image of size ({x}, {y}) from {path}\")\n",
    "    h = w = 512\n",
    "    image = transforms.CenterCrop(min(x,y))(image)\n",
    "    image = image.resize((w, h), resample=Image.Resampling.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.config = \"./configs/pnp_refine_all.yaml\"\n",
    "        self.img_dir_path = \"/hdd3/junyi/DiffSketch/data\"\n",
    "        self.text_dir_path = \"PATH_TO_CLASS_INFO\"\n",
    "        self.ddim_eta = 0.0\n",
    "        self.save_all_features = True\n",
    "        self.H = 512\n",
    "        self.W = 512\n",
    "        self.C = 4\n",
    "        self.f = 8\n",
    "        self.model_config = \"configs/stable-diffusion/v1-inference.yaml\"\n",
    "        self.ckpt = \"models/ldm/stable-diffusion-v1/model.ckpt\"\n",
    "        self.precision = \"autocast\"\n",
    "        self.check_safety = False\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/ldm/stable-diffusion-v1/model.ckpt\n",
      "Global Step: 840000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'text_projection.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "setup_config = OmegaConf.load(\"./configs/pnp/setup.yaml\")\n",
    "model_config = OmegaConf.load(f\"{opt.model_config}\")\n",
    "model = load_model_from_config(model_config, f\"{opt.ckpt}\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "unet_model = model.model.diffusion_model\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "img_dir_path=opt.img_dir_path\n",
    "img_paths=[]\n",
    "\n",
    "# iterate through all images in the directory\n",
    "for root, dirs, files in os.walk(img_dir_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "            path = os.path.join(root, file)\n",
    "            img_paths.append(path)\n",
    "\n",
    "img_path=img_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyi/miniconda3/envs/od/lib/python3.9/site-packages/pytorch_lightning/utilities/seed.py:60: UserWarning: -1 is not in bounds, numpy accepts from 0 to 4294967295\n",
      "  rank_zero_warn(f\"{seed} is not in bounds, numpy accepts from {min_seed_value} to {max_seed_value}\")\n",
      "Global seed set to 546653476\n",
      "WARNING:root:Experiment directory already exists, previously saved content will be overriden\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded input image of size (1024, 1024) from /hdd3/junyi/DiffSketch/data/25.jpg\n",
      "Running DDIM inversion with 999 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Inversion: 100%|█████████▉| 995/999 [01:15<00:00, 13.08it/s]"
     ]
    }
   ],
   "source": [
    "exp_config = OmegaConf.load(opt.config)\n",
    "experiment_name =  exp_config.config.experiment_name+img_path.split(\"/\")[-1].split(\".\")[0] # experiment_name + \"01.jpg\"\n",
    "exp_path_root = setup_config.config.exp_path_root\n",
    "\n",
    "exp_config.config.seed = -1\n",
    "exp_config.config.prompt = \"\"\n",
    "exp_config.config.scale = 1.0\n",
    "    \n",
    "seed = exp_config.config.seed \n",
    "seed_everything(seed)\n",
    "\n",
    "save_feature_timesteps = exp_config.config.ddim_steps if exp_config.config.init_img == '' else exp_config.config.save_feature_timesteps\n",
    "\n",
    "outpath = f\"{exp_path_root}/{experiment_name}\"\n",
    "\n",
    "# if os.path.exists(outpath):\n",
    "#     continue\n",
    "\n",
    "callback_timesteps_to_save = [save_feature_timesteps]\n",
    "if os.path.exists(outpath):\n",
    "    logging.warning(\"Experiment directory already exists, previously saved content will be overriden\")\n",
    "    if exp_config.config.init_img != '':\n",
    "        with open(os.path.join(outpath, \"args.json\"), \"r\") as f:\n",
    "            args = json.load(f)\n",
    "        callback_timesteps_to_save = args[\"save_feature_timesteps\"] + callback_timesteps_to_save\n",
    "\n",
    "\n",
    "sample_path = os.path.join(outpath, \"inversion\")\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "\n",
    "# save parse_args in experiment dir\n",
    "with open(os.path.join(outpath, \"args.json\"), \"w\") as f:\n",
    "    args_to_save = OmegaConf.to_container(exp_config.config)\n",
    "    args_to_save[\"save_feature_timesteps\"] = callback_timesteps_to_save\n",
    "    json.dump(args_to_save, f)\n",
    "\n",
    "def ddim_sampler_callback(pred_x0, x_t, i):\n",
    "    save_feature_maps_callback(i)\n",
    "    # save_sampled_img(pred_x0, i, predicted_samples_path)\n",
    "\n",
    "def save_feature_maps(blocks, i, feature_type=\"input_block\"):\n",
    "    block_idx = 0\n",
    "    for block in tqdm(blocks, desc=\"Saving input blocks feature maps\"):\n",
    "        if not opt.save_all_features and block_idx < 4:\n",
    "            block_idx += 1\n",
    "            continue\n",
    "        if \"ResBlock\" in str(type(block[0])):\n",
    "            if opt.save_all_features or block_idx == 4:\n",
    "                save_feature_map(block[0].in_layers_features, f\"{feature_type}_{block_idx}_in_layers_features_time_{i}\")\n",
    "                save_feature_map(block[0].out_layers_features, f\"{feature_type}_{block_idx}_out_layers_features_time_{i}\")\n",
    "        if len(block) > 1 and \"SpatialTransformer\" in str(type(block[1])):\n",
    "            save_feature_map(block[1].transformer_blocks[0].attn1.k, f\"{feature_type}_{block_idx}_self_attn_k_time_{i}\")\n",
    "            save_feature_map(block[1].transformer_blocks[0].attn1.q, f\"{feature_type}_{block_idx}_self_attn_q_time_{i}\")\n",
    "        block_idx += 1\n",
    "\n",
    "def save_feature_maps_callback(i):\n",
    "    if opt.save_all_features:\n",
    "        save_feature_maps(unet_model.input_blocks, i, \"input_block\")\n",
    "    save_feature_maps(unet_model.output_blocks , i, \"output_block\")\n",
    "\n",
    "def save_feature_map(feature_map, filename):\n",
    "    memory_storage[filename] = feature_map\n",
    "\n",
    "# cls_name=\"cat\" # TODO, pass the cls_name from text\n",
    "cls_name = None\n",
    "\n",
    "assert exp_config.config.prompt is not None\n",
    "prompts = [exp_config.config.prompt.replace('object', cls_name)] if cls_name else [exp_config.config.prompt]\n",
    "\n",
    "precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            uc = model.get_learned_conditioning([\"\"])\n",
    "            if isinstance(prompts, tuple):\n",
    "                prompts = list(prompts)\n",
    "            c = model.get_learned_conditioning(prompts)\n",
    "            shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "\n",
    "            z_enc = None\n",
    "            if exp_config.config.init_img != '':\n",
    "                assert os.path.isfile(img_path)\n",
    "                init_image = load_img(img_path).to(device)\n",
    "                init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "                ddim_inversion_steps = 999\n",
    "                z_enc, _ = sampler.encode_ddim(init_latent, num_steps=ddim_inversion_steps, conditioning=c,unconditional_conditioning=uc,unconditional_guidance_scale=exp_config.config.scale)\n",
    "            else:\n",
    "                z_enc = torch.randn([1, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "            torch.save(z_enc, f\"{outpath}/z_enc.pt\")\n",
    "            samples_ddim, _ = sampler.sample(S=exp_config.config.ddim_steps,\n",
    "                            conditioning=c,\n",
    "                            batch_size=1,\n",
    "                            shape=shape,\n",
    "                            verbose=False,\n",
    "                            unconditional_guidance_scale=exp_config.config.scale,\n",
    "                            unconditional_conditioning=uc,\n",
    "                            eta=opt.ddim_eta,\n",
    "                            x_T=z_enc,\n",
    "                            img_callback=ddim_sampler_callback,\n",
    "                            callback_ddim_timesteps=save_feature_timesteps,\n",
    "                            outpath=outpath)\n",
    "\n",
    "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "            x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "            if opt.check_safety:\n",
    "                x_samples_ddim = check_safety(x_samples_ddim)\n",
    "            x_image_torch = torch.from_numpy(x_samples_ddim).permute(0, 3, 1, 2)\n",
    "\n",
    "            sample_idx = 0\n",
    "            for x_sample in x_image_torch:\n",
    "                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                img.save(os.path.join(sample_path, f\"{sample_idx}.png\"))\n",
    "                sample_idx += 1\n",
    "\n",
    "print(f\"Sampled images and extracted features saved in: {outpath}\")\n",
    "exp_path_root_config = OmegaConf.load(\"./configs/pnp/setup.yaml\")\n",
    "exp_path_root = exp_path_root_config.config.exp_path_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversion done, start refinement\n",
    "# read seed from args.json of source experiment\n",
    "with open(os.path.join(exp_path_root, experiment_name, \"args.json\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "    seed = args[\"seed\"]\n",
    "    source_prompt = args[\"prompt\"]\n",
    "negative_prompt = source_prompt if exp_config.negative_prompt is None else exp_config.negative_prompt\n",
    "\n",
    "seed_everything(seed)\n",
    "possible_ddim_steps = args[\"save_feature_timesteps\"]\n",
    "assert exp_config.num_ddim_sampling_steps in possible_ddim_steps or exp_config.num_ddim_sampling_steps is None, f\"possible sampling steps for this experiment are: {possible_ddim_steps}; for {exp_config.num_ddim_sampling_steps} steps, run 'run_features_extraction.py' with save_feature_timesteps = {exp_config.num_ddim_sampling_steps}\"\n",
    "ddim_steps = exp_config.num_ddim_sampling_steps if exp_config.num_ddim_sampling_steps is not None else possible_ddim_steps[-1]\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=opt.ddim_eta, verbose=False) \n",
    "\n",
    "seed = torch.initial_seed()\n",
    "opt.seed = seed\n",
    "\n",
    "outpaths = [os.path.join(f\"{exp_path_root}/{experiment_name}/refined\")]\n",
    "out_label = f\"INJECTION_T_{exp_config.feature_injection_threshold}_STEPS_{ddim_steps}\"\n",
    "out_label += f\"_NP-ALPHA_{exp_config.negative_prompt_alpha}_SCHEDULE_{exp_config.negative_prompt_schedule}_NP_{negative_prompt.replace(' ', '_')}\"\n",
    "\n",
    "predicted_samples_paths = [os.path.join(outpath, f\"predicted_samples_{out_label}\") for outpath in outpaths]\n",
    "for i in range(len(outpaths)):\n",
    "    os.makedirs(outpaths[i], exist_ok=True)\n",
    "    os.makedirs(predicted_samples_paths[i], exist_ok=True)\n",
    "    # save args in experiment dir\n",
    "    with open(os.path.join(outpaths[i], \"args.json\"), \"w\") as f:\n",
    "        json.dump(OmegaConf.to_container(exp_config), f)\n",
    "\n",
    "def ddim_sampler_callback(pred_x0, xt, i):\n",
    "    # save_sampled_img(pred_x0, i, predicted_samples_paths)\n",
    "    pass\n",
    "\n",
    "def load_feature_map(filename):\n",
    "    return memory_storage[filename]\n",
    "\n",
    "def load_target_features():\n",
    "    self_attn_output_block_indices = [4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    out_layers_output_block_indices = [4]\n",
    "    output_block_self_attn_map_injection_thresholds = [ddim_steps // 2] * len(self_attn_output_block_indices)\n",
    "    feature_injection_thresholds = [exp_config.feature_injection_threshold]\n",
    "    target_features = []\n",
    "\n",
    "    time_range = np.flip(sampler.ddim_timesteps)\n",
    "    total_steps = sampler.ddim_timesteps.shape[0]\n",
    "\n",
    "    iterator = tqdm(time_range, desc=\"loading source experiment features\", total=total_steps)\n",
    "\n",
    "    for i, t in enumerate(iterator):\n",
    "        current_features = {}\n",
    "        for (output_block_idx, output_block_self_attn_map_injection_threshold) in zip(self_attn_output_block_indices, output_block_self_attn_map_injection_thresholds):\n",
    "            if i <= int(output_block_self_attn_map_injection_threshold):\n",
    "                output_q = load_feature_map(f\"output_block_{output_block_idx}_self_attn_q_time_{t}\")\n",
    "                output_k = load_feature_map(f\"output_block_{output_block_idx}_self_attn_k_time_{t}\")\n",
    "                current_features[f'output_block_{output_block_idx}_self_attn_q'] = output_q\n",
    "                current_features[f'output_block_{output_block_idx}_self_attn_k'] = output_k\n",
    "\n",
    "        for (output_block_idx, feature_injection_threshold) in zip(out_layers_output_block_indices, feature_injection_thresholds):\n",
    "            if i <= int(feature_injection_threshold):\n",
    "                output = load_feature_map(f\"output_block_{output_block_idx}_out_layers_features_time_{t}\")\n",
    "                current_features[f'output_block_{output_block_idx}_out_layers'] = output\n",
    "\n",
    "        target_features.append(current_features)\n",
    "\n",
    "    return target_features # for one timestep, attn: (16, patch**2, dim); out_layers: (2, 1280, 16, 16)\n",
    "\n",
    "batch_size = len(exp_config.prompts)\n",
    "prompts = exp_config.prompts\n",
    "assert prompts is not None\n",
    "\n",
    "start_code_path = f\"{exp_path_root}/{experiment_name}/z_enc.pt\"\n",
    "start_code = torch.load(start_code_path).cuda() if os.path.exists(start_code_path) else None\n",
    "if start_code is not None:\n",
    "    start_code = start_code.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "injected_features = load_target_features()\n",
    "unconditional_prompt = \"\"\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            uc = None\n",
    "            nc = None\n",
    "            if exp_config.scale != 1.0:\n",
    "                uc = model.get_learned_conditioning(batch_size * [unconditional_prompt])\n",
    "                nc = model.get_learned_conditioning(batch_size * [negative_prompt])\n",
    "            if not isinstance(prompts, list):\n",
    "                prompts = list(prompts)\n",
    "            c = model.get_learned_conditioning(prompts)\n",
    "            shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                            conditioning=c,\n",
    "                                            negative_conditioning=nc,\n",
    "                                            batch_size=len(prompts),\n",
    "                                            shape=shape,\n",
    "                                            verbose=False,\n",
    "                                            unconditional_guidance_scale=exp_config.scale,\n",
    "                                            unconditional_conditioning=uc,\n",
    "                                            eta=opt.ddim_eta,\n",
    "                                            x_T=start_code,\n",
    "                                            img_callback=ddim_sampler_callback,\n",
    "                                            injected_features=injected_features,\n",
    "                                            negative_prompt_alpha=exp_config.negative_prompt_alpha,\n",
    "                                            negative_prompt_schedule=exp_config.negative_prompt_schedule,\n",
    "                                            )\n",
    "\n",
    "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "            x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "            if opt.check_safety:\n",
    "                x_samples_ddim = check_safety(x_samples_ddim)\n",
    "            x_image_torch = torch.from_numpy(x_samples_ddim).permute(0, 3, 1, 2)\n",
    "\n",
    "            sample_idx = 0\n",
    "            for k, x_sample in enumerate(x_image_torch):\n",
    "                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                # img.save(os.path.join(outpaths[k], f\"{out_label}_sample_{sample_idx}.png\"))\n",
    "                img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
